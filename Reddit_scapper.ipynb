{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Scrapping comments data from r/wallstreetbets using Pushshift\n",
    "### modified code from https://github.com/shergreen/wallstreetbets_sentiment_analysis\n",
    "\n",
    "### Check the comments dataset scraped using this python file here: https://www.kaggle.com/ssliao/wallstreetbets-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import string\n",
    "import datetime as dt\n",
    "\n",
    "# slightly edited version of code presented here:\n",
    "# https://www.osrsbox.com/blog/2019/03/18/watercooler-scraping-an-entire-subreddit-2007scape/\n",
    "\n",
    "PUSHSHIFT_REDDIT_URL = \"https://api.pushshift.io/reddit\"\n",
    "\n",
    "def get_date(created):\n",
    "    if created:\n",
    "        return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = str(text).replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def fetchObjects(**kwargs):\n",
    "    # Default paramaters for API query\n",
    "    params = {\n",
    "        \"sort_type\":\"created_utc\",\n",
    "        \"sort\":\"asc\",\n",
    "        \"size\":100\n",
    "        }\n",
    "\n",
    "    # Add additional paramters based on function arguments\n",
    "    for key,value in kwargs.items():\n",
    "        params[key] = value\n",
    "\n",
    "    # Print API query paramaters\n",
    "    print(params)\n",
    "\n",
    "    # Set the type variable based on function input\n",
    "    # The type can be \"comment\" or \"submission\", default is \"comment\"\n",
    "    type = \"comment\"\n",
    "    if 'type' in kwargs and kwargs['type'].lower() == \"submission\":\n",
    "        type = \"submission\"\n",
    "    \n",
    "    # Perform an API request\n",
    "    r = requests.get(PUSHSHIFT_REDDIT_URL + \"/search/\" + type, params=params)\n",
    "    print(r)\n",
    "\n",
    "    # Check the status code, if successful, process the data\n",
    "    if r.status_code == 200:\n",
    "        response = json.loads(r.text)\n",
    "        data = response['data']\n",
    "        sorted_data_by_id = sorted(data, key=lambda x: int(x['id'],36))\n",
    "        return sorted_data_by_id\n",
    "\n",
    "def extract_reddit_data(**kwargs):\n",
    "    # Specify the start timestamp 2020, 2, 1\n",
    "    max_created_utc = math.floor(datetime(2020,2,1).timestamp())\n",
    "    max_id = 0\n",
    "    # specify the features\n",
    "    columns = ['author', 'author_created_utc', 'author_flair_css_class',\n",
    "       'author_flair_text', 'author_fullname', 'body', 'can_gild',\n",
    "       'controversiality', 'created_utc', 'distinguished', 'gilded', 'id',\n",
    "       'link_id', 'nest_level', 'parent_id', 'reply_delay', 'retrieved_on',\n",
    "       'score', 'stickied', 'subreddit', 'subreddit_id', 'mod_removed',\n",
    "       'edited', 'user_removed']\n",
    "\n",
    "    # Open a file for JSON output\n",
    "    fn = kwargs['subreddit'] + \"_\" + kwargs['type'] + \".csv\"\n",
    "    if(path.exists(fn)):\n",
    "        \n",
    "        existing_data = pd.read_csv(fn)[columns]\n",
    "        most_recent = existing_data['created_utc'].idxmax()\n",
    "        recent_created_utc = int(existing_data['created_utc'][most_recent])\n",
    "        recent_id = int(existing_data['id'][most_recent],36)\n",
    "        if(recent_created_utc > max_created_utc):\n",
    "            max_created_utc = recent_created_utc - 1\n",
    "            max_id = recent_id\n",
    "\n",
    "    else:\n",
    "        existing_data = pd.DataFrame(columns = columns)\n",
    "        existing_data.to_csv(fn)   \n",
    "    \n",
    "    \n",
    "\n",
    "    # While loop for recursive function\n",
    "    while 1:\n",
    "#     while max_created_utc <= math.floor(datetime(2020,6,1).timestamp()):\n",
    "        nothing_processed = True\n",
    "        # Call the recursive function\n",
    "        objects = fetchObjects(**kwargs,after=max_created_utc)\n",
    "        new_df = pd.DataFrame(columns = columns)\n",
    "        # Loop the returned data, ordered by date\n",
    "        for object in objects:\n",
    "            \n",
    "            id = int(object['id'],36)\n",
    "            if id > max_id:\n",
    "                nothing_processed = False\n",
    "                created_utc = object['created_utc']\n",
    "                max_id = id\n",
    "                if created_utc > max_created_utc: \n",
    "                    max_created_utc = created_utc\n",
    "                    if object['author'] == '[deleted]' and object['body'] == '[deleted]':\n",
    "                        # remove deleted comments\n",
    "                        continue\n",
    "                    else:\n",
    "                        # output new comments to a DataFrame  \n",
    "                        new_df = new_df.append(object,ignore_index=True)\n",
    "               \n",
    "        \n",
    "        print('The newest comments are at: ', get_date(max_created_utc))\n",
    "        new_df['body'] = new_df['body'].apply(remove_punctuations)\n",
    "        new_df = new_df[columns]\n",
    "        print('Extracted ', new_df.shape[0], 'comments with ', new_df.shape[1], ' features.')\n",
    "        # append the new dataframe to opened csv file\n",
    "        new_df.to_csv(fn, mode='a',header=False)        \n",
    "        \n",
    "        # Exit if nothing happened\n",
    "        if nothing_processed: return\n",
    "        max_created_utc -= 1\n",
    "\n",
    "        # Sleep a little before the next recursive function call\n",
    "        time.sleep(1)\n",
    "\n",
    "# Start program by calling function with:\n",
    "# 1) Subreddit specified\n",
    "# 2) The type of data required (comment or submission)\n",
    "# ex: extract_reddit_data(subreddit=\"wallstreetbets\",type=\"comment\")\n",
    "\n",
    "# useful codes for removing abnormal data when it fails to find most_recent = existing_data['created_utc'].idxmax()\n",
    "# mask = pd.to_numeric(df['created_utc'], errors='coerce').isna()\n",
    "# print(mask.sum())\n",
    "# df.drop(df[mask == True].index, inplace = True)\n",
    "extract_reddit_data(subreddit=\"wallstreetbets\",type=\"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
